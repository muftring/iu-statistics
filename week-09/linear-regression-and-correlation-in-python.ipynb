{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression and Correlation\n",
    "What is the association between two quantitative variables?   \n",
    "What is the strength of association (correlation) between the two variables?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explanatory variable**: independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**response variable**: dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use a regression equation to predict the value of the response variable from the explanatory variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let $y$ denote the response variable\n",
    "- let $x$ denote the explanatory variable\n",
    "\n",
    "We want to underatand the relationship between $y$ and $x$, and explain it using a mathematical equation. In this sense, $y$ being the response variable, we can say that $y$ depends on $x$, or $x$ dictates the value of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for a straight line:\n",
    "\n",
    "$$y = \\alpha + \\beta x$$\n",
    "\n",
    "The **slope** is how \"steep\" the line is and is denoted by $\\beta$.  \n",
    "The **intercept** is where the line crosses the y-axis (when $x = 0$) and is denoted by $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solving for $\\alpha$   \n",
    "and $\\beta$: \"rise over run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positive Relationship**: $\\beta > 0$   \n",
    "**Negative Relationahip**: $\\beta < 0$   \n",
    "**Independence**: $\\beta = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "We are interested in the relationship between a response variable (violent crimes at the state level) and some explanatory (independent) variables:\n",
    "- poverty rate\n",
    "- perentage of the population living in urban areas\n",
    "- percentage of residents who are high school graduates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is $y$? the dependent variable: *violent crimes at the state level*\n",
    "\n",
    "What is $x$? the explanatory variables, any of: poverty rate, urban population percentage, high school graduate percentage.\n",
    "\n",
    "The key is that we are explaining differences in violent crimes at the state level (the response, or dependent variable) using variation in the different independent variables.\n",
    "\n",
    "There is an \"art\" in selecting the response variable in studies. It can often be argued either way when choosing. Be careful of *correlations* vs. *causation*, and whether a relationship between variables really explains or provides a causal relationahip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working the example of *violent crimes at the state level*, we have:\n",
    "- poverty rate: $y = 210 + 25x$\n",
    "- perentage of the population living in urban areas: $y = 26 + 8x$\n",
    "- percentage of residents who are high school graduates: $y = 1756 - 16x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we interpret the equations?\n",
    "- poverty rate: $y = 210 + 25x$\n",
    "  - The y-intercept is 210, when $x = 0$, meaning there is no poverty, the violent crime rate is equal to 210 at the state level\n",
    "  - There is a positive relationahip, a positive slope, relating $x$ (independent variable: poverty) with $y$ (dependent variable: violent crime rate at the state level)\n",
    "  - this means that a 1-unit increase in poverty rate (explanatory variable) is associated with a 25-unit increase in violent crime rate at the state level (response variable)\n",
    "\n",
    "- perentage of the population living in urban areas: $y = 26 + 8x$\n",
    "  - a positive relationship\n",
    "  - a 1-unit increase in population living in urban areas is associated with an 8-unit increase in violent crime rate at the state level\n",
    "\n",
    "- percentage of residents who are high school graduates: $y = 1756 - 16x$\n",
    "  - a negative relationship\n",
    "  - the violent crime rates at a high school graduation rate of zero is 1756\n",
    "  - there is a decreasing relationship\n",
    "  - a 1-unit decrease in the percentage of residents who are high school graduates is associated (on average) with a 16-unit increase in violent crime rate at the state level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you do if there is no data point that shows a zero value for a variable in the dataset? How do you obtain the y-intercept if there is no data showing the $y$ value when $x$ is zero? As we are using a line to relate the two variales, the line can be extended to find the y-intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the questions imply about causality?\n",
    "\n",
    "These equations do not imply anything about causality. We cannot state, for example, that if we increase percentage of residents who are high school graduates we will decrease the violent crime rate at the state level. These are not relationships that are causal in any way. The only thing we can state is, given the data this is the type of relationship that we observe overall. But we cannot make any claims of one variable causing an effect on the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Prediction Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension to linear regression, finding a line that best describes the relationship between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: generate a scatterplot**\n",
    "- look at how the data relates\n",
    "- does it look like we can draw a straight line through the data points?\n",
    "- are there any non-linear curves? it is inappropriate to use a Straight Line Model when there is a non-linear relationship\n",
    "- a **box-and-whisker plot** along the x-axis can also help visually perceive the density or distribution\n",
    "- there may be \"distant\" data points, or outliers, which appear to be well outside the bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Equation\n",
    "The prediction equation for a line:\n",
    "\n",
    "$$\\hat{y} = a+bx$$\n",
    "\n",
    "Y-hat denotes the fact that we use sample data to estimate the slope and intercept for the equation. Using this equation we can obtain a predicted value of the response variable for any given of $x$ as long as we know the slope and intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Equation for the Best Striaght Line\n",
    "\n",
    "To obtain the y-intercept $a$:\n",
    "\n",
    "$$a = \\bar{y} + b \\bar{x}$$\n",
    "\n",
    "where $\\bar{y}$ denotes the average value of the response variable (dependent variable)   \n",
    "and $\\bar{x}$ is the average value of $x$ across all data points in your sample\n",
    "\n",
    "to obtain the slope $b$:\n",
    "\n",
    "$$b = \\frac{\\sum (x - \\bar{x})(y - \\bar{y})}{\\sum (x - \\bar{x})^{2}}$$\n",
    "\n",
    "In the numerator we are summing the product of the difference of all $x$ values and the average $x$ value $\\bar{x}$, and the difference of all $y$ values and the average $y$ value $\\bar{y}$.\n",
    "\n",
    "The denominator is obtained by taking the sum of the squared deviation of each observation of $x$ from the overall mean $\\bar{x}$.\n",
    "\n",
    "Executing this for every data point, we can obtain the appropriate y-intercept and slope for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of Outliers on the Prediction Equation\n",
    "Outliers exist in almost every dataset.\n",
    "\n",
    "It is a judgement call whether to include or exclude outliers in the dataset when creating a Straight Line Model. It could be an objective decision based on some firm criteria, or it could be a subjective decision based on less quantifiable criteria.\n",
    "\n",
    "Recall that the mean value is sensitive to outliers, and so removing outliers from a dataset will have an impact to the prediction equations (above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Errors are called Residuals\n",
    "\n",
    "How good is our prediction equation?\n",
    "\n",
    "For any value of $x$, how far off would we tend to be in predicting the $y$ value compared to actual $y$ values in the dataset?\n",
    "\n",
    "The difference between an observed value and the predicted value of that response variable, $y - \\hat{y}$, is called the residual (or the error term). It can be thought of what's left over, that is unexplained variation between a data point and what is predicted based on the model (the $y$ prediction equation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Residuals\n",
    "Compare actual $y$ values with $y$ values produced by prediction equation.\n",
    "\n",
    "Example,\n",
    "- take an actual $x$ value from the dataset (e.g., Poverty Rate)\n",
    "- plug the actual $x$ value into the prediction equation: $\\hat{y} = a + b\\hat{x}$\n",
    "- to get the predicted $\\hat{y}$ value (predicted response variable, e.g. Murder Rate) associated with the $x$ value (explanatory variable, e.g. Poverty Rate) substitute the value of $x$ in the equation\n",
    "- the value will differ some because the prediction equation will follow the line, and the actual values will be slightly diferent from the predicted values\n",
    "\n",
    "Larger residuals will be observed when outliers are included in the prediction equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Equation has Least Squares Property\n",
    "\n",
    "The equations given for $a$ and $b$ are the values that provide the prediction equation $\\hat{y} = a + bx$ for which the residual sum of squares, $SSE = \\sum(y-\\hat{y})^{2}$, is a minimum.\n",
    "\n",
    "Note: SSE = sum of squared errors\n",
    "\n",
    "The residual sum of squares describes the variation of the data around the prediction line.\n",
    "\n",
    "The least squares line prediction has the following properties:\n",
    "- the sum and the mean of the residuals equals 0\n",
    "- the line passes through the point $(\\bar{x}, \\bar{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a prediction equation for a line, $y = \\alpha + \\beta x$, each value of $x$ corresponds to a single value of $y$. \n",
    "\n",
    "In real life, however, not all observations with the same $x$-value have the same $y$-value. Rather, there is a conditional probability distribution over the $y$ values for a fixed $x$ value which allows for variability in $y$ for each value of $x$. \n",
    "\n",
    "For a given value of $x$, $\\alpha + \\beta x$ represents the mean of the conditional probability distribution of $y$ for subjets having that value of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Function\n",
    "\n",
    "$$E(y) = \\alpha + \\beta x$$\n",
    "\n",
    "This shows the relationship between $x$ and the mean of the conditional distribution of $y$. It is called a linear regression function because it uses a straight line to relate the mean of $y$ to the values of $x$.\n",
    "\n",
    "A **regression function** is a mathematical function that describes how the mean of a response variable changes according to the value of an explanatory variable.\n",
    "\n",
    "**Regression coefficients**: the intercept and the slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing Variation about the Regression Line\n",
    "\n",
    "We can describe the variabbility of the $y$-values for all subjects having the same $x$-value. This is the *conditional standard deviation*, or $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Square Error: Estimating Conditional Variation\n",
    "\n",
    "Linear regeression assumes that the standard deviation of the conditional distribution of $y$ is:\n",
    "- identical at the variaous values of $x$\n",
    "- normally distributed around $x$\n",
    "\n",
    "$$s = \\sqrt{\\frac{SSE}{n-2}} = \\sqrt{\\frac{\\sum(y - \\hat{y})^{2}}{n-2}}$$\n",
    "\n",
    "At any fixed value of $x$, our model predicts that $y$ varies around a mean $E(y)$ with a standard deviation $s$.\n",
    "\n",
    "There are two degrees of freedom, the $n-2$ term, because we have two unknowns: $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal and Conditional Distributions\n",
    "- The *marginal distribution* shows the overall variability in $y$ values.\n",
    "- The *conditional distribution* shows how $y$ varies at fixed value of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Linear Association: The Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope in a regression line tells us the direction of an association between two variables, but not its strength.\n",
    "\n",
    "Correlation tells us about the **strength of linear association** between two variables. \n",
    "\n",
    "The slope of a line, $b$, depends on the units of measurement.\n",
    "\n",
    "Correlation is the value the slope would take if both $x$ and $y$ variables had equal standard deviations. In other words, it standardizes the measure of association so that they do not depend on the unit of meaurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Correlation\n",
    "We can calculate the standard deviation of $x$ and $y$\n",
    "\n",
    "$$s_{x} = \\sqrt{\\frac{\\sum(x - \\bar{x})^{2}}{n-1}}$$\n",
    "\n",
    "$$s_{y} = \\sqrt{\\frac{\\sum(y - \\bar{y})^{2}}{n-1}}$$\n",
    "\n",
    "The correlation, denoted by $r$, relates to the slope $b$ of the prediction equation by:\n",
    "\n",
    "$$r = \\left ( \\frac{s_{x}}{s_{y}} \\right ) b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the Correlation\n",
    "- measures the **strenth of linear association** between $x$ and $y$\n",
    "- correlation must fall between -1 and 1, $-1 \\le r \\le 1$\n",
    "  - a correlation of -1 occurs when two variables are perfectly negatively correlated with each other\n",
    "  - a correlation of 1 occurs when two variables are perfectly positively correlated with each other\n",
    "- correlation has the same sign as the slope\n",
    "  - if two variables are negatively correlated, that means on average when you see an increase in one of the variables you see a decrease in the other\n",
    "  - a positive sign on the slope means a positive sign on the correlation, so when you see an increase in one variable you tend to also see an increase in the other variable\n",
    "- when $b = 0$, $r = 0$\n",
    "- $r = \\pm 1$ when all sample points fall exactly on the prediction line.\n",
    "  - They correspond to perfect positive and negative linear associations where there is no prediction error.\n",
    "- the larger the absolute value of $r$, the stronger the linear association\n",
    "- correlation treats $x$ and $y$ Symmetrically, unlike the slope\n",
    "- value of $r$ does not depend on the variables' units of observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation is Useful\n",
    "Correlation is useful because it allows us to compare the strength of association across multiple variables.\n",
    "\n",
    "Example:\n",
    "- we can have two linear regression equations\n",
    "  - one for the relationship between murder rate and poverty rate\n",
    "  - $y = 210 + 25x$\n",
    "  - and another for the relatioship between murder rate and percentage of residents who are high school graduates\n",
    "  - $y = 1756 - 16x$\n",
    "- just looking at these equations, we cannot tell whether highschool graduation rates or poverty rate is more strongly associated with murder rates\n",
    "- but if the correlation bbetween murder rate and poverty is 0.63, and the correlation between murder rate and high school graduation rates is -0.30, we know that the strength of association between merder rate and poverty is higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Squared: How well can $x$ predict $y$?\n",
    "We want to know how well our regression equation performs: how well can $x$ predict $y$? To what extent does variation in $x$ predict variation in $y$?\n",
    "\n",
    "One way to assess this is by looking at the **r-squared** statistic, which measures the proportional reduction in prediction error that we get by modelling $y$ from $x$, rather than just using the average of $y$ as a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule 1** (predicting $y$ without using $x$): the best predictor is $\\bar{y}$, the sample mean.\n",
    "\n",
    "$$E_{1} = TSS = \\sum (y - \\bar{y})^{2}$$\n",
    "\n",
    "TSS = Total Sum of Squares\n",
    "\n",
    "**Rule 2** (predicting $y$ using $x$): when the relationship between $x$ and $y$ is linear, the prediction equation $\\hat{y} = a + bx$ provides the best predictor of $y$. For each subject, substituting the $x$-value into this equation provides the predicted value of $y$.\n",
    "\n",
    "$$E_{2} = SSE = \\sum (y - \\hat{y})^{2}$$\n",
    "\n",
    "SSE = Sum of Square Error, or Residual Sum of Squares\n",
    "\n",
    "(Agresti, 4ed, Chapter 9, section 4, pp. 273-274; see specifically figure 9.13 and surrounding descriptive text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportional reduction in error from using the linear prediction equation instead of $\\bar{y}$ (the sample mean) to predict $y$ is:\n",
    "\n",
    "$$r^{2} = \\frac{TSS - SSE}{TSS} = \\frac{\\sum (y - \\bar{y})^{2} - \\sum (y - \\hat{y})^{2}}{\\sum (y - \\bar{y})^{2}}$$\n",
    "\n",
    "This is known as **r-squared**, or the **coefficient of determination**.\n",
    "\n",
    "R-squared is the square of the correlation.\n",
    "\n",
    "The square of the correlation basically tells us the percent of variation explained when you use $x$ to predict $y$ rather than using an average value of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting R-Squared\n",
    "(see example 9.9 in textbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of R-Squared\n",
    "- r-squared falls between 0 and 1\n",
    "- if there is no prediction error, then r-squared = 1\n",
    "- r-squared = 0 when there is no relationship (b = 0) between $x$ and $y$\n",
    "- r-squared measures the strength of linear association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for The Slope and Correlation\n",
    "Assumptions for Statistical Inference:\n",
    "- the study uses randomization, such as simple random sample in a survey\n",
    "- the mean of $y$ is related to $x$ by the linear equation: $E(y) = \\alpha + \\beta x$\n",
    "- the conditional standard deviation $\\sigma$ is identical at each $x$-value\n",
    "- the condirtional distribution of $y$ at each value of $x$ is normal\n",
    "\n",
    "According to the first assumption the data represent a random sample, whereas the second assumption implies that the linear regression function is valid. These two are the most important of the four assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of Independence\n",
    "If the normal conditional distribution of $y$ is the same at each $x$ value, then the two quantitative variables are statistically independent. For the regression function $E(y) = \\alpha + \\beta x$, this means that the slope $\\beta$ is zero. The null hypothesis is that the variables are statistically independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test independence against $H_{a}: \\beta \\ne 0$, or a one-sided alternative to predict the direction of the association. The test statistic equals:\n",
    "\n",
    "$$t = \\frac{b}{se}$$\n",
    "\n",
    "The formula for the standard error is:\n",
    "\n",
    "$$se = \\frac{s}{\\sqrt{\\sum (x - \\bar{x})^{2}}}$$\n",
    "where\n",
    "$$s = \\sqrt{\\frac{SSE}{n-2}}$$\n",
    "\n",
    "A small $s$ occurs when the data points show little variability about the prediction equation. Also, the standard error of $b$ is inversely related to $\\sqrt{\\sum (x - \\bar{x})^{2}}$. This sum incxreases as the sample size increases. The $se$ also decreases when the $x$-values are more highly spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The P-value for $H_{a}: \\beta \\ne 0$ is the two-tail probability from the t-distribution. For large $df$ (degrees of freedom), the t-distribution is similar to the standard normal, so the P-value can be approximated using the normal probability table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval for the Slope\n",
    "A small P-value for $H_{0}: \\beta = 0$ suggests that the regression line has a non-zero slope. A confidence interval for $\\beta$ has the formula:\n",
    "\n",
    "$$b \\pm t(se)$$\n",
    "\n",
    "Recall, the null hypothesis is asserting $\\beta = 0$, and so if 0 does not fall within the resulting confidence interval we may reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for the Correlation\n",
    "The test statistic for testing $H_{0}: \\rho = 0$ is:\n",
    "\n",
    "$$t = \\frac{r}{\\sqrt{(1-r^{2})/(n-2)}}$$\n",
    "\n",
    "This provides the same value as the test statistic $t = \\frac{b}{se}$, since both test essentially the same hypothesis, with the same degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assumptions and Violations\n",
    "(see lecture video and Agresti chapter 9.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
