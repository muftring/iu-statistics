{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multiple Regression Model\n",
    "\n",
    "Until now we have modeled the relationship between two variables (the response variable and the explanatory variable). We call this *bivariate* because there are two variables involved:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1}$$\n",
    "\n",
    "Now we consider the situation in which we have more than one explanatory variable to explain the variation in the response variable. \n",
    "\n",
    "Suppose there are two explanatory variables $x_{1}$ and $x_{2}$. The bivariate regression function can be generalized to the multiple regression funmction:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1} + \\beta_{2}x_{2}$$\n",
    "\n",
    "This equation specifies the population mean of $y$ for all subjects with certain values of $x_{1}$ and $x_{2}$. When there are additional $x$ varaibles, each has a slope coefficient $\\beta_{x}$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multiple Regression Function\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1} + \\beta_{2}x_{2}$$\n",
    "\n",
    "The equation that controls for $x_{2}$ is also called a **partial regression equation**, as it focuses on a part of the observations (in the example, it is the counties having $x_{2} = 50$).\n",
    "\n",
    "> *control for...* keep a variabkle value constant\n",
    "\n",
    "**Simposon's Paradox**: when a partial association has the opposite direction relative to a bivarite association (this is observed as a change in the sign of the coefficient, this can also be seen in a scatterplot).\n",
    "\n",
    "Some Notes about Regression Coefficients:\n",
    "- In multiple regression, a slope describes the effect of an explanatory variable while *controlling* the effects of other explanatory variables.\n",
    "- in bivariate regression, a slope describes the effect of an explanatory variable while *ignoring* all other possible explanatory variables\n",
    "- the beta coefficient describes the change in the mean of $y$ for a one unit increase in that predictor\n",
    "- the multivariate regression equation (that we considered in the prededing example) assumes that the slope of the partial relationship between $y$ and each predictor is identical for all combinations of values of the other explanatory variables. This means that the model is appropriate when there is not statistical interaction between the explanatory variables\n",
    "- a partial slope in a multiple regression equation usually differs from the slope in the bivariate model for that predictor, but it would not if it uncorrelated with the other explanatory variables in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Equation and Residuals\n",
    "\n",
    "The prediction equation that estimates the multiple regression model:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{k}x_{k}$$\n",
    "\n",
    "is denoted by:\n",
    "\n",
    "$$\\hat{y} = a + b_{1}x_{1} + b_{2}x_{2} + ... + b_{k}x_{k}$$\n",
    "\n",
    "We get the predicted value for $y$ for a subject by substituting the $x$-values for that subject into the prediction equation.\n",
    "\n",
    "The regression model has *residuals* that measure prediction errors. For a subject with predicted response $\\hat{y}$ and observed response $y$, the residual is $y - \\hat{y}$.\n",
    "\n",
    "A statistic accociated with a regression equation is the residual sum of squares, which summarizes the closeness of the prediction to the response data.\n",
    "\n",
    "The Residual Sum of Squares (SSE) is expressed as:\n",
    "\n",
    "$$SSE = \\sum (y - \\hat{y})^2$$\n",
    "\n",
    "The SSE measures the unexplained variation in the response variable, after running the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot Matrix for Bivariate Relationships\n",
    "\n",
    "Plots pf the variables against each other provide an informal check of the relationship pattern between the two variables. One way to visualize the relationships between pairs of variables is through a **scatterplot matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Regression Plots\n",
    "\n",
    "When there is one main explanatory variable and several control variables, then an informative way to visualize the relationship between the response and the main explanatory variable is through partial regression plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Correlation and $R^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Determination ($R^{2}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties for $R$ and $R^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity with Many Explanatory Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
