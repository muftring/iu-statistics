{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multiple Regression Model\n",
    "\n",
    "Until now we have modeled the relationship between two variables (the response variable and the explanatory variable). We call this *bivariate* because there are two variables involved:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1}$$\n",
    "\n",
    "Now we consider the situation in which we have more than one explanatory variable to explain the variation in the response variable. \n",
    "\n",
    "Suppose there are two explanatory variables $x_{1}$ and $x_{2}$. The bivariate regression function can be generalized to the multiple regression funmction:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1} + \\beta_{2}x_{2}$$\n",
    "\n",
    "This equation specifies the population mean of $y$ for all subjects with certain values of $x_{1}$ and $x_{2}$. When there are additional $x$ varaibles, each has a slope coefficient $\\beta_{x}$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multiple Regression Function\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1} + \\beta_{2}x_{2}$$\n",
    "\n",
    "The equation that controls for $x_{2}$ is also called a **partial regression equation**, as it focuses on a part of the observations (in the example, it is the counties having $x_{2} = 50$).\n",
    "\n",
    "> *control for...* keep a variabkle value constant\n",
    "\n",
    "**Simposon's Paradox**: when a partial association has the opposite direction relative to a bivarite association (this is observed as a change in the sign of the coefficient, this can also be seen in a scatterplot).\n",
    "\n",
    "Some Notes about Regression Coefficients:\n",
    "- In multiple regression, a slope describes the effect of an explanatory variable while *controlling* the effects of other explanatory variables.\n",
    "- in bivariate regression, a slope describes the effect of an explanatory variable while *ignoring* all other possible explanatory variables\n",
    "- the beta coefficient describes the change in the mean of $y$ for a one unit increase in that predictor\n",
    "- the multivariate regression equation (that we considered in the prededing example) assumes that the slope of the partial relationship between $y$ and each predictor is identical for all combinations of values of the other explanatory variables. This means that the model is appropriate when there is not statistical interaction between the explanatory variables\n",
    "- a partial slope in a multiple regression equation usually differs from the slope in the bivariate model for that predictor, but it would not if it uncorrelated with the other explanatory variables in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Equation and Residuals\n",
    "\n",
    "The prediction equation that estimates the multiple regression model:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{k}x_{k}$$\n",
    "\n",
    "is denoted by:\n",
    "\n",
    "$$\\hat{y} = a + b_{1}x_{1} + b_{2}x_{2} + ... + b_{k}x_{k}$$\n",
    "\n",
    "We get the predicted value for $y$ for a subject by substituting the $x$-values for that subject into the prediction equation.\n",
    "\n",
    "The regression model has *residuals* that measure prediction errors. For a subject with predicted response $\\hat{y}$ and observed response $y$, the residual is $y - \\hat{y}$.\n",
    "\n",
    "A statistic accociated with a regression equation is the residual sum of squares, which summarizes the closeness of the prediction to the response data.\n",
    "\n",
    "The Residual Sum of Squares (SSE) is expressed as:\n",
    "\n",
    "$$SSE = \\sum (y - \\hat{y})^2$$\n",
    "\n",
    "The SSE measures the unexplained variation in the response variable, after running the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot Matrix for Bivariate Relationships\n",
    "\n",
    "Plots pf the variables against each other provide an informal check of the relationship pattern between the two variables. One way to visualize the relationships between pairs of variables is through a **scatterplot matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Regression Plots\n",
    "\n",
    "When there is one main explanatory variable and several control variables, then an informative way to visualize the relationship between the response and the main explanatory variable is through partial regression plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Correlation and $R^{2}$\n",
    "In the case of bivariate regression, we saw that the correlation $r$ and its square $r^{2}$ describe the strength of linerar association for bivariate relationships.\n",
    "\n",
    "Analogous measures for multivariate regressions are the $R$ and $R^{2}$ (R-Squared) measures.\n",
    "\n",
    "R is also called multiple correlation. It is the correlation between the observed $y$-values and the predicted $\\hat{y}$-values.\n",
    "\n",
    "$R$ always falls between 0 and 1.\n",
    "\n",
    "$R^{2}$ is also called the coefficient of multiple regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Determination ($R^{2}$)\n",
    "\n",
    "$R^{2}$ summarizes the relative improvement in predictions using the prediction equation instead of $\\bar{y}$.\n",
    "\n",
    "How to calculate $R^{2}$:\n",
    "- First, assuming that prediction for $y$ is simply $\\bar{y}$, calculate the prediction errors for each observation\n",
    "- square each prediction error and sum all such squared terms. The resulting statistic is called the **total sum of squares**:\n",
    "\n",
    "$$TSS = \\sum (y - \\bar{y})^{2}$$\n",
    "\n",
    "- Second, calculate the predictions as per the multivariate regression equation, and calculate the prediction of errors for each observation\n",
    "- square the prediction errors and sum across all such squared terms\n",
    "- the resulting statistic is called the **residual sum of squares** (*sum of squared errors*):\n",
    "\n",
    "$$SSE = \\sum (y - \\hat{y})^{2}$$\n",
    "\n",
    "- $R^{2}$ is the proportional reduction in error from using the prediction equation instead of $\\bar{y}$. It is calculated as:\n",
    "\n",
    "$$R^{2} = \\frac{TSS - SSE}{TSS} = \\frac{\\sum (y - \\bar{y})^{2} - \\sum (y - \\hat{y})^{2}}{\\sum (y - \\bar{y})^{2}}$$\n",
    "\n",
    "- $R^{2}$ measures the proportion of total variation in $y$ that is explained by the predictive power of all the explanatory variables, through the multiple regression model\n",
    "\n",
    "> Explanation:\n",
    ">\n",
    "> \"Using *variable-1* and *variable-2* (... and *variable-n*) together to predict *dependent-variable* provides $R^{2}$*-as-a-percent* reduction in the prediction error relative to only using $\\bar{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Correlation and $R^{2}$\n",
    "The squre root of $R^{2}$ is $R$, the multiple correlation coefficient.\n",
    "\n",
    "$R$ is the correlation between the predictions obtained from the multiple regression model and the observed values of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties for $R$ and $R^{2}$\n",
    "- $R^{2}$ falls between 0 and 1\n",
    "- The larger the value of $R^{2}$, the better the set of explanatory variables that collectively predict $y$\n",
    "- $R^{2} = 1$ only when all the residuals are 0, that is when all $y = \\hat{y}$ (all observations equal all predictions), so that SSE = 0\n",
    "- $R^{2} = 0$ when the predictions do not vary as any of the $x$-values vary. In that case:\n",
    "  - $R^{2}$ cannot decrease when we add an explanatory variable to the model. It is impossible to explain less variation by adding more variables to the model.\n",
    "  - $R^{2}$ for the multiuple regression model is at least as large as the $r^{2}$ values for the separate bivariate models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity with Many Explanatory Variables\n",
    "When there are many explanatory variables and the correlations among them are strong, and once you have included a few of them in the model, then $R^{2}$ does not increase much more when you add additional variables. When $R^{2}$ does not increase much, this does not mean that the additional variables are uncorrelated with $y$. It means merely that they don't add much new power for explaining $y$, given the predictors in the model. Such a situation is quite common in social sciences and is referred to as **multicollinearity**.\n",
    "\n",
    "Ideally we should use explanatory variables that have weak correlations with each other, but strong correlations with $y$. In practice this is not always possible.\n",
    "\n",
    "In practice, the sample size needed to do a multiple regression gets larger when you want to use more explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for Multiple Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiple regression function describes the relationship the explanatory variables and the mean of the response variables:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1}+ ... \\beta_{k}x_{k}$$\n",
    "\n",
    "To make valid inferences about the parameters in the above equation, we need a set of assumptions:\n",
    "\n",
    "- the population distribution of $y$ is normal, for each combination of values $x_{1}$, $x_{2}$, ..., $x_{k}$\n",
    "- the standard devialtion $\\sigma$ pf the conmditional distribution of responses on $y$ is the same at each combination of values $x_{1}$, $x_{2}$, ..., $x_{k}$\n",
    "- the sample is randomly selected\n",
    "\n",
    "In practice, the above assumptions are never satisfied perfectly.\n",
    "\n",
    "Two-sided inferences are robbust to the normality and common $\\sigma$ assumptions.\n",
    "\n",
    "More important are the assumptions of randomization and that the regression functions describes well how the mean of $y$ depends on the explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Collective Influence of the Explanatory Variables\n",
    "We can check whether the explanatory variables collectively have a statistically signifiant effect on the response variable by testing:\n",
    "\n",
    "$$H_{0}: \\beta_{1} = \\beta_{2} = ... = \\beta_{k} = 0$$\n",
    "\n",
    "This states that $y$ is statistically indepnendent of all $k$ explanatory variables. The alternative hypothesis is:\n",
    "\n",
    "$$H_{a}: At \\: least \\: one \\: \\beta_{i} \\ne 0$$\n",
    "\n",
    "This states that at least one explanatory variable is related to $y$.\n",
    "\n",
    "The preceding hypothesis are equivalent to:\n",
    "\n",
    "- $H_{0}$: population multiple correlation = 0\n",
    "- $H_{a}$: population multiple correlation > 0 \n",
    "\n",
    "For the above hypotheses about the $k$ predictors, the test statistic equals:\n",
    "\n",
    "$$F = \\frac{R^{2}/k}{(1 - R^{2})/[n - (k + 1)]}$$\n",
    "\n",
    "The sampling distribution of this test statistic is called the **F distribution**.\n",
    "\n",
    "The shape of the F distribution is determined by two degrees of freeedom terms, denoted $df_{1}$ and $df_{2}$\n",
    "\n",
    "- $df_{1} = k$, the number of explanatory variables in the model\n",
    "- $df_{2} = n - (k + 1)$, which is $n$ - the number of parameters in regression equation, representing the $k$ beta terms and the alpha terms (the +1)\n",
    "\n",
    "The mean of the F distribution is approximately 1. Thus, values of the F-statistic that are significantly greater than 1 provide stronger evidence against $H_{0}$ (the null hypothesis).\n",
    "\n",
    "Under the assumption that $H_{0}$ is true, the P-value is the probability that the F-statistic is larger than the observed F-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Individual Regression Coefficients\n",
    "We can also conduct tests for indiviual regression coefficients. If we are interested in whether or not an individual explanatory variable $x_{i}$ is statistically significantly predicting variation in the explanatory variable, we can conduct a separate test for that. \n",
    "\n",
    "Consider an arbitrary explanatory variable $x_{i}$, with coefficient $\\beta_{i}$ in the multiple regression model. The test for its partial effect on $y$ has\n",
    "\n",
    "$$H_{0}: \\beta_{i} = 0$$\n",
    "\n",
    "The alternative can be two-sided:\n",
    "\n",
    "$$H_{a}: \\beta_{i} \\ne 0$$\n",
    "\n",
    "or one-sided:\n",
    "\n",
    "$$H_{a}: \\beta_{i} \\gt 0$$\n",
    "\n",
    "or\n",
    "\n",
    "$$H_{a}: \\beta_{i} \\lt 0$$\n",
    "\n",
    "The test statistic can be calculated using the sample estimate $b_{i}$:\n",
    "\n",
    "$$t = \\frac{b_{i}}{se}$$\n",
    "\n",
    "If the $H_{0}$ is true and the model assumptions hold, the t-statistic has the t distribution with $df = n - (k + 1)$\n",
    "\n",
    "A confidence interval for $\\beta_{i}$ is:\n",
    "\n",
    "$$b_{i} \\pm t(se)$$\n",
    "\n",
    "The t-score comes from the t-table, with $df = n - (k + 1)$. For example, a 95% confidence interval for the partial effect of $x_{1}$ is $b_{1} \\pm t_{.025}(se)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional notes about the F-statistic\n",
    "The F-statistic is a ratio of *Mean Squares*\n",
    "\n",
    "$$F = \\frac{Regression \\: mean \\: square}{Residual \\: mean \\: square}$$\n",
    "\n",
    "The **regression mean square** equals the regression sum of squares divided by its degrees of freedom ($k$). The **residual mean square** equals the sum of squared errors divided by its degrees of freedom ($n - (k + 1)$). \n",
    "\n",
    "The F-statistic and the t-statistic are related. The square of the t-statistic for teting that a partial regression coefficient equals zero is an F test statistic having the F distribution with $df_{1} = 1$ and $df_{2} = n - (k + 1)$.\n",
    "\n",
    "In general, if a statistic has a t distribution with $d$ degrees of freedom, then the square of that statistic has the F distribution with $df_{1} = 1$ and $df_{2} = d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions between Predictors in their Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiple regression equation:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{k}x_{k}$$\n",
    "\n",
    "assumes that the partial relationship between $y$ and each $x_{i}$ is linear and that the slope $\\beta_{i}$ of that relationship is identical for all values of the other explanatory variables.\n",
    "\n",
    "the above model is sometimes too simple to be adequate. Often there is an **interaction** with the relationship between two variables changing according to the value of a third variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Product Terms\n",
    "\n",
    "A common approach for allowing interaction introduces cross-product terms of the explanatory variables into the multiple regression model. With two explanatory variables, the model is:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}x_{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing an Interaction Term\n",
    "For a model with two explanatory variables and an interaction term, the regression equation is:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}x_{2}$$\n",
    "\n",
    "In the simpler model which assumes no interaction, $\\beta_{3} = 0$. Thus the hypothesis for no interaction is:\n",
    "\n",
    "$$H_{0}: \\beta_{3} = 0$$\n",
    "\n",
    "The t-statistic is:\n",
    "\n",
    "$$t = \\frac{b_{3}}{se}$$\n",
    "\n",
    "If the P-value for $H_{a}$ is very high (e.g., 0.51), then there is little evidence of interaction of $x_{1}$ and $x_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering the Explanatory Variables\n",
    "When the interaction term as included in the model along with the main effects individually, the coefficients of $x_{1}$ and $x_{2}$ are not usually meaningful because they refer to the effect of a predictor only when the other predictor equals zero. \n",
    "\n",
    "An alternative weay to parameterize the interaction model so that the main effects can be interpreted when $x_{1}$ and $x_{2}$ are at their mean level is to center the explanatory variables by subtracting the mean.\n",
    "\n",
    "Let $x^{C}_{1} = x_{1} - \\mu_{x_{1}}$ and $x^{C}_{2} = x_{2} - \\mu_{x_{2}}$, so that each new explanatory variable has a mean of zero. Then we can express the interaction model as:\n",
    "\n",
    "$$E(y) = \\alpha + \\beta_{1}x^{C}_{1} + \\beta_{2}x^{C}_{2} + \\beta_{3}x^{C}_{1}x^{C}_{2} $$\n",
    "\n",
    "$$= \\alpha + \\beta_{1}(x_{1} - \\mu_{x_{1}}) + \\beta_{2}(x_{2} - \\mu_{x_{2}}) + \\beta_{3}(x_{1} - \\mu_{x_{1}})(x_{2} - \\mu_{x_{2}})$$\n",
    "\n",
    "Now $\\beta_{1}$ refers to the effect of $x_{1}$ at the mean of $x_{2}$ and $\\beta_{2}$ refers to the effect of $x_{2}$ at the mean of $x_{1}$.\n",
    "\n",
    "When we rerun the interaction model ... after centering the predictors about their sample means, the estimate for the interaction term is the same as for the uncentered model.\n",
    "\n",
    "However, the estiamtes for the effects of $x_{1}$ and $x_{2}$ are similar to the values for the no interaction model. This happens because the coefficient for a variable represents its effect at the mean of the other variable, which is typically similar to the effect for the no interaction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
